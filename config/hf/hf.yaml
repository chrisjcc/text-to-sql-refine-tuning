model:
  name: meta-llama/Llama-3.1-8B-Instruct
  cache_dir: ./cache
  trust_remote_code: true
  device_map: auto

  # Attention implementation
  # Options: "auto" (try flash_attention_2, fallback to default), "flash_attention_2", "sdpa", "eager", null
  # "auto" is recommended - uses flash-attn if available, falls back gracefully if not
  attn_implementation: auto

  # Torch dtype for non-quantized loading
  torch_dtype: bfloat16

tokenizer:
  padding_side: left
  add_eos_token: true
  add_bos_token: false
  use_fast: true

hub:
  push_to_hub: false
  repo_name: chrisjcc
  private: false
  token: null  # Will use HF_TOKEN from env
