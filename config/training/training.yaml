# GRPO Training Configuration
output_dir: ./outputs
run_name: grpo-text-to-sql

# Model & LoRA
use_peft: true
peft:
  # LoRA configuration
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: none
  task_type: CAUSAL_LM

  # QLoRA configuration
  use_qlora: true
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true

# Model compilation (optional, PyTorch 2.0+)
torch_compile: false

# Batch Configuration
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
num_train_epochs: 3
max_steps: -1

# Learning Rate & Schedule
learning_rate: 5.0e-6
lr_scheduler_type: cosine
warmup_steps: 100
warmup_ratio: 0.03

# Generation Parameters
max_new_tokens: 256
temperature: 0.7
top_p: 0.9
do_sample: true

# GRPO-Specific
num_generations: 4
kl_coef: 0.05
gamma: 1.0

# Optimization
# Note: bf16 should be FALSE when using quantization
# The bnb_4bit_compute_dtype=bfloat16 in quantization config already handles precision
# Setting bf16=true enables PyTorch AMP which conflicts with quantized model dtypes
bf16: false
gradient_checkpointing: true
optim: adamw_torch

# Saving & Logging
save_strategy: steps
save_steps: 500
save_total_limit: 3
logging_steps: 10

# Evaluation
evaluation_strategy: steps
eval_steps: 500

# Environment Configuration
environment:
  type: text_to_sql
  prompt_template: instructional
  include_schema: true
  max_schema_length: 1024
  few_shot_examples: 0  # Increase for few-shot learning

  # Response handling
  max_response_length: 512
  stop_sequences: ["\n\n", "<|eot_id|>", "Question:"]

  # Reward shaping (optional)
  reward_shaping:
    enabled: false
    min_reward: 0.0
    max_reward: 1.0
    normalize: false

# GRPO Advanced Parameters
grpo_advanced:
  # Async generation for faster sampling
  use_async_generation: false
  async_batch_size: 4

  # Value function for advantage estimation
  use_value_function: false
  value_loss_coef: 0.1

  # Clipping
  cliprange: 0.2
  cliprange_value: 0.2

  # Reference model
  use_reference_model: true
  reference_free: false

# Stop sequences for generation
stop_sequences:
  - "\n\n"
  - "<|eot_id|>"
  - "Question:"
  - "Schema:"
